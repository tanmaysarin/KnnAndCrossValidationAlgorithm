{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4w4C9qMucYv"
   },
   "source": [
    "# **Lab 1: k-Nearest Neighbours and Cross Validation**\n",
    "\n",
    "CS 412, Spring 2021\n",
    "\n",
    "***This is an individual lab, i.e., NOT for group work.***\n",
    "\n",
    "This is your first lab. You will see how to use k-Nearest Neighbours for classification tasks. You will also learn how to do model selection by cross validation. In particular, we will help you to get started with these algorithms _step by step_.\n",
    "\n",
    "***Deadline:***\n",
    "This assignment is due **Feb 10** (Anywhere on Earth, [AoE](https://www.timeanddate.com/time/zones/aoe)). That is, you can resubmit as often as you like provided that anywhere on Earth is still on or before this date. \n",
    "\n",
    "***How to submit:***\n",
    "See bottom of the page\n",
    "\n",
    "***Python version:***\n",
    "The code should work on Python 3.7 or later, though it might work on earlier versions (not tested). There should be no version problem if you work on Colab.  See a more detailed introduction to Python and Colab at this [link](https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb#scrollTo=nxvEkGXPM3Xh).  \n",
    "\n",
    "**Please note before starting the lab:**\n",
    "\n",
    "1. If you use Colab, copy this file to your own Google Drive so that you can edit it.\n",
    "\n",
    "2. Since the experiments involve randomness, it is important to ensure that your results are replicable. To this end, your implementation should take one integer (or any numeric value) as a seed that is used to initialize the random number generators.\n",
    "See, e.g. [random.seed](https://docs.python.org/3/library/random.html).\n",
    "This has been done for you in the first code block below.\n",
    "\n",
    "3. <font color='red'> There are unit test cases provided after most functions you need to implement. Make good use of them.</font>  For numerical results (i.e., not discrete values), it is fine if your result is within 1% relative difference from the reference result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hutIH7_uyvjZ"
   },
   "source": [
    "## Problem 1: Implementation of the k-Nearest Neighbours (kNN) classifier and Cross-Validation {-}\n",
    "\n",
    "In Problem 1, you will implement kNN and cross validation from scratch, which is a good exercise to make sure that you fully understand those algorithms.\n",
    "Do not use any library such as scikit-learn that already has kNN or cross validation implemented.\n",
    "But you can use general libraries for array and matrix operations such as numpy.\n",
    "\n",
    "##1.1 Implementation of the k-Nearest Neighbours classifier\n",
    "\n",
    "\n",
    "\n",
    "**Step 1. (10 points)** The kNN classifier mainly consists of two stages:\n",
    "\n",
    "1.   During training, the classifier takes the training data and simply stores it.\n",
    "2.   During testing, kNN classifies every test example $x$ by \n",
    "\n",
    "> i) finding the $k$ training examples that are most similar to $x$;\n",
    "\n",
    "> ii) outputing the most common label among these $k$ examples.\n",
    "\n",
    "To measure the similarity between samples, we commonly compute the Euclidean distance. The Euclidean distance (a.k.a. $L_2$ distance) between two examples $p$ and $q$ in an $n$-dimensional space is defined as the square root of:\n",
    "\n",
    "\\begin{equation}\n",
    "(p_1-q_1)^2 + (p_2-q_2)^2 + ... + (p_n-q_n)^2. \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "This term is equal to \n",
    "\\begin{equation}\n",
    "\\sum_i p_i^2 + \\sum_i q_i^2 - 2 \\sum_i p_i q_i. \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "With Euclidean distance, the smaller the value, the more similar the two examples are. Actually, there are many different ways to measure the similarity, such as cosine distance, Manhattan, Chebyshev, and Hamming distance. In practice, you can choose the one that suits your problem. For this lab, we will implement Equation (2) with a function `my_euclidean_dist` that  computes the Euclidean distances.\n",
    "\n",
    "**DO NOT use np.linalg.norm() or function from scipy.**\n",
    "\n",
    "**Unit test:** to unit test `my_euclidean_dist`, you can construct two matrices by yourself, e.g., `X_train` being 3-by-2 and `X_test` being 2-by-2. Then you can compute the squared Euclidean distances by hand, and compare it with the result of your code.  See the last four lines of the following code block, which lie outside the definition of `my_euclidean_dist`.  You can uncomment them for testing, but comment them back when you finish the entire lab.\n",
    "\n",
    "`euclidean_dist` will be called eventually by the `knn_predict` function in Step 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWDHxYKZGV1B"
   },
   "outputs": [],
   "source": [
    "# set up code for this experiment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukpJSEIfBzt6"
   },
   "outputs": [],
   "source": [
    "def my_euclidean_dist(X_test, X_train):\n",
    "  \"\"\"\n",
    "  Compute the *squared* distance between each test example and each training example\n",
    "\n",
    "  Input:\n",
    "  - X_test: A numpy array of shape (num_test, dim_feat) containing test data\n",
    "  - X_train: A numpy array of shape (num_train, dim_feat) containing training data\n",
    "\n",
    "  Output:\n",
    "  - dists: A numpy array of shape (num_test, num_train) where \n",
    "           dist[i, j] is the squared Euclidean distance between \n",
    "           the i-th test example and the j-th training example\n",
    "  \"\"\"\n",
    "  num_test = X_test.shape[0]\n",
    "  num_train = X_train.shape[0]\n",
    "  dists = np.zeros((num_test, num_train))\n",
    "  # TODO:\n",
    "  # Compute the squared L2 distance between all test and training examples.\n",
    "  #\n",
    "  # One most straightforward way is to use nested for loop\n",
    "  # to iterate over all test and training samples.\n",
    "  # Here is the pseudo-code:\n",
    "  # for i = 0 ... num_test - 1\n",
    "  #    a[i] = square of the norm of the i-th test example\n",
    "  # for j = 0 ... num_train - 1\n",
    "  #    b[j] = square of the norm of the j-th training example\n",
    "  # for i = 0 ... num_test - 1\n",
    "  #    for j = 0 ... num_train - 1\n",
    "  #        dists[i, j] = a[i] + b[j] - 2 * np.dot(i-th test example, j-th training example)\n",
    "  # return dists\n",
    "  \n",
    "  \n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** \n",
    "\n",
    "  return dists\n",
    "\n",
    "# Unit test code here (you can uncomment the four lines below to test)\n",
    "# Compute by hand to check if the result is correct.\n",
    "# The right matrix of squared distance should be\n",
    "# [[ 8 10  1]\n",
    "#  [ 2  8  9]]\n",
    "# X_train = np.array([[1, 2], [0, 3], [-1, 1]])\n",
    "# X_test = np.array([[-1, 0], [2, 1]])\n",
    "# my_dists = my_euclidean_dist(X_test, X_train)\n",
    "# print(my_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aEEdL4Oww8O"
   },
   "source": [
    "However, you can entirely avoid using loops by reformulating Equation (2) with linear algebra.  The trick is to reformulate the L2 distance as two broadcast sums and matrix multiplication.  \n",
    "\n",
    "**Task:** Try the following implementation and feel the speedup!  Understand the following implementation.  You do not need to write down your understanding or submit anything for it, but it will be helpful to understand it.\n",
    "\n",
    "**Note:**  Since Euclidean distance computation underlies all the subsequent experiments, its efficiency is highly important. Therefore, in the sequel, we will NOT use `my_euclidean_dist` that you just implemented.  Instead, we will use `euclidean_dist`.  However, your implementation of `my_euclidean_dist` will still be graded based on unit test; it will need to be copied to `Lab_1.py` (see submission instruction at the bottom of the page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XsyQxBYxBvp"
   },
   "outputs": [],
   "source": [
    "def euclidean_dist(X_test, X_train):\n",
    "  dists = np.add(np.sum(X_test ** 2, axis=1, keepdims=True), np.sum(X_train ** 2, axis=1, keepdims=True).T) - 2* X_test @ X_train.T\n",
    "  return dists\n",
    "\n",
    "# Unit test code here (you can uncomment the four lines below to test)\n",
    "# X_train = np.array([[1, 2], [0, 3], [-1, 1]])\n",
    "# X_test = np.array([[-1, 0], [2, 1]])\n",
    "# dists = euclidean_dist(X_test, X_train)\n",
    "# print(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_aUcYK-KINy"
   },
   "source": [
    "**Step 2. (10 points)**  Once distances are calculated, we can find the top $k$ nearest neighbors for each test example by retrieving from the dists matrix. \n",
    "In particular, for each test example $x$, we can sort all the training examples by their distance to $x$ then find the $k$ most nearest neighbors.  \n",
    "\n",
    "**HINT**: Recall from the lecture that `argsort` is useful for this purpose.\n",
    "\n",
    "**Note**: to run the unit test, you need to uncomment the unit test in the previous code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lveUni0IT2G4"
   },
   "outputs": [],
   "source": [
    "def find_k_neighbors(dists, Y_train, k):\n",
    "  \"\"\"\n",
    "  find the labels of the top k nearest neighbors\n",
    "\n",
    "  Inputs:\n",
    "  - dists: distance matrix of shape (num_test, num_train)\n",
    "  - Y_train: A numpy array of shape (num_train) containing ground true labels for training data\n",
    "  - k: An integer, k nearest neighbors\n",
    "\n",
    "  Output:\n",
    "  - neighbors: A numpy array of shape (num_test, k), where each row containts the \n",
    "               labels of the k nearest neighbors for each test example\n",
    "  \"\"\"\n",
    "  # TODO:\n",
    "  # find the top k nearest neighbors for each test sample.\n",
    "  # retrieve the corresponding labels of those neighbors.\n",
    "  # Here is the pseudo-code:\n",
    "  # for i = 0 ... num_test-1\n",
    "  #     idx = numpy.argsort(i-th row of dists)\n",
    "  #     neighbors[i] = Y_train(idx[0]), ..., Y_train(idx[k-1])\n",
    "  # return neighbors\n",
    "  # Advanced: You can accelerate the code by, e.g., argsort on the `dists` matrix directly\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return neighbors\n",
    "\n",
    "# Unit test code here (you can uncomment the lines below to test)\n",
    "# Compute by hand to check if the result is correct.\n",
    "# k = 3   # you can vary it as 1 or 3\n",
    "# Y_train = np.array([0, 1, 1])\n",
    "# neighbors = find_k_neighbors(dists, Y_train, k)\n",
    "# print(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISP74Eh2rr4i"
   },
   "source": [
    "**Step 3. (10 points)** Finally, we can put together `euclidean_dist` and `find_k_neighbors`, so that labels can be predicted for test examples.  In kNN, we take the labels of the $k$ nearest neighbors and find the most common one and assign it to the test sample.\n",
    "\n",
    "**Hint:** You may find [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html#numpy-unique) and `argmax` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXqx94yVt41e"
   },
   "outputs": [],
   "source": [
    "def knn_predict(X_test, X_train, Y_train, k):\n",
    "  \"\"\"\n",
    "  predict labels for test data.\n",
    "\n",
    "  Inputs:\n",
    "  - X_test: A numpy array of shape (num_test, dim_feat) containing test data.\n",
    "  - X_train: A numpy array of shape (num_train, dim_feat) containing training data.\n",
    "  - Y_train: A numpy array of shape (num_train) containing ground true labels for training data\n",
    "  - k: An integer, k nearest neighbors\n",
    "\n",
    "  Output:\n",
    "  - Y_pred: A numpy array of shape (num_test). Predicted labels for the test data.\n",
    "  \"\"\"\n",
    "  # TODO:\n",
    "  # find the labels of k nearest neighbors for each test example,\n",
    "  # and then find the majority label out of the k labels\n",
    "  #\n",
    "  # Here is the pseudo-code:\n",
    "  # dists = euclidean_dist(X_test, X_train)\n",
    "  # neighbors = find_k_neighbors(dists, Y_train, k)\n",
    "  # Y_pred = np.zeros(num_test, dtype=int)  # force dtype=int in case the dataset\n",
    "  #                                         # stores labels as float-point numbers\n",
    "  # for i = 0 ... num_test-1\n",
    "  #     Y_pred[i] = # the most common/frequent label in neighbors[i], you can\n",
    "  #                 # implement it by using np.unique\n",
    "  # return Y_pred\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return Y_pred\n",
    "\n",
    "# Unit test code here (you can uncomment the lines below to test)\n",
    "# Compute by hand to check if the result is correct.\n",
    "# Y_pred = knn_predict(X_test, X_train, Y_train, k)\n",
    "# print(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EAaUOyC4AY"
   },
   "source": [
    "**Step 4. (5 points)** Once we obtain the predicted labels, we need to implement a function to compare them against the true label and compute the error rate in percentage (i.e., a number between 0 and 100). In the following code block, implement the `compute_error_rate` function by following the specified inputs and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr3PVo1_C7_r"
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(ypred, ytrue):\n",
    "  \"\"\"\n",
    "  Compute error rate given the predicted results and true lable.\n",
    "  Inputs:\n",
    "  - ypred: array of prediction results.\n",
    "  - ytrue: array of true labels.\n",
    "    ypred and ytrue should be of same length.\n",
    "  Output:\n",
    "  - error rate: float number indicating the error in percentage\n",
    "                (i.e., a number between 0 and 100).\n",
    "  \"\"\"\n",
    "  # Here is the pseudo-code:\n",
    "  # err = 0\n",
    "  # for i = 0 ... num_test - 1\n",
    "  #     err = err + (ypred[i] != ytrue[i])  # generalizes to multiple classes\n",
    "  # error_rate = err / num_test * 100\n",
    "  # return error_rate\n",
    "  #\n",
    "  # Advanced (optional): \n",
    "  #   implement it in one line by using vector operation and the `mean` function\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdQoqFW9khl9"
   },
   "source": [
    "## 1.2 Splitting training data for cross validation {-}\n",
    "\n",
    "Cross validation is a technique in which we train our model using a subset of the available dataset and then evaluate using the complementary subset of the data. In this assignment, we use the $n$-fold cross validation method to perform cross validation. In $n$-fold cross validation, we evenly partition the dataset into $n$ mutually disjoint subsets (a.k.a. _folds_). We train an ML model on all but one subset (i.e., train on the union of $n-1$ folds), and then evaluate the model on the subset that was left out.  The former is called _training subset_, while the latter is called _validation subset_. This process is repeated $n$ times, with a different subset reserved for evaluation (and excluded from training) each time. If the size of the dataset is not exactly divisible by $n$, the remainder can be arbitrarily distributed into the folds.\n",
    "\n",
    "**Step 1. (10 points)** In the following code block, you will need to implement a function that partitions the dataset in to training sets and validation sets. The output should be lists of indices which indicate the training examples and validation examples.  Function inputs and outputs are detailed in the code block. \n",
    "\n",
    "**Hint:** You may find random permutation useful here: [np.random.permutation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.permutation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY6j6Ggdr1ph"
   },
   "outputs": [],
   "source": [
    "def split_nfold(num_examples, n):\n",
    "  \"\"\"\n",
    "  Split the dataset in to training sets and validation sets.\n",
    "  Inputs:\n",
    "  - num_examples: Integer, the total number of examples in the dataset\n",
    "  - n: number of folds\n",
    "  Outputs:\n",
    "  - train_sets: List of lists, where train_sets[i] (i = 0 ... n-1) contains \n",
    "                the indices of examples for training\n",
    "  - validation_sets: List of list, where validation_sets[i] (i = 0 ... n-1) \n",
    "                contains the indices of examples for validation\n",
    "\n",
    "  Example:\n",
    "  When num_examples = 10 and n = 5, \n",
    "    the output train_sets should be a list of length 5, \n",
    "    and each element in this list is itself a list of length 8, \n",
    "    containing 8 indices in 0...9\n",
    "  For example, \n",
    "    we can initialize by randomly permuting [0, 1, ..., 9] into, say,\n",
    "      [9, 5, 3, 0, 8, 4, 2, 1, 6, 7]\n",
    "    Then we can have\n",
    "    train_sets[0] = [3, 0, 8, 4, 2, 1, 6, 7],  validation_sets[0] = [9, 5]\n",
    "    train_sets[1] = [9, 5, 8, 4, 2, 1, 6, 7],  validation_sets[1] = [3, 0]\n",
    "    train_sets[2] = [9, 5, 3, 0, 2, 1, 6, 7],  validation_sets[2] = [8, 4]\n",
    "    train_sets[3] = [9, 5, 3, 0, 8, 4, 6, 7],  validation_sets[3] = [2, 1]\n",
    "    train_sets[4] = [9, 5, 3, 0, 8, 4, 2, 1],  validation_sets[4] = [6, 7]\n",
    "  Within train_sets[i] and validation_sets[i], the indices do not need to be sorted.\n",
    "  \"\"\"\n",
    "  # Here is the pseudo code:\n",
    "  # idx = np.random.permutation(num_examples).tolist() # generate random index list\n",
    "  # fold_size = num_examples//n   # compute how many examples in one fold.\n",
    "  #                               # note '//' as we want an integral result\n",
    "  # train_sets = []\n",
    "  # validation_sets = []\n",
    "  # for i = 0 ... n-1\n",
    "  #\t  start = # compute the start index of the i-th fold\n",
    "  #\t  end = # compute the end index of the i-th fold\n",
    "  #   if i == n-1\n",
    "  #     end = num_examples  # handle the remainder by allocating them to the last fold\n",
    "  #   For example, when num_examples = 11 and n = 5, \n",
    "  #     fold_size = 11//5 = 2\n",
    "  #     i = 0: start = 0, end = 2\n",
    "  #     i = 1: start = 2, end = 4\n",
    "  #     i = 2: start = 4, end = 6\n",
    "  #     i = 3: start = 6, end = 8\n",
    "  #     i = 4: start = 8, end = 11  (take up the remainder of 11//5)\n",
    "  #\n",
    "  #   # Now extract training example indices from the idx list using start and end\n",
    "  #   train_set = idx[`0 to num_example-1` except `start to end-1`]  \n",
    "  #   train_sets.append(train_set)\n",
    "  #\n",
    "  #   # Extract validation example indices from the idx list using start and end\n",
    "  #   val_set = idx[start to end-1] \n",
    "  #   validation_sets.append(val_set)\n",
    "  \n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return train_sets, validation_sets\n",
    "\n",
    "# Unit test code here (you can uncomment the lines below to test)\n",
    "# train_sets, val_sets = split_nfold(11, 5)\n",
    "# print(train_sets[4])\n",
    "# print(val_sets[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LWlt0cd8S4n"
   },
   "source": [
    "**Step 2. (10 points)** Next, you will need to implement the `cross_validation` function, which will output the cross validation error rate. You may want to call previously defined functions such as `split_nfold` and `compute_error_rate`. In this function, you need to loop over each of the $n$ training/validation partitions in the output of `split_nfold`.\n",
    "Then perform training on train_sets[i] and compute the test error on validation_sets[i].  The final cross validation error rate is the average error rate over all partitions. \n",
    "\n",
    "To improve generality, `cross_validation` takes as its first input argument a generic _classifier_ function.  In this lab, we will use kNN, and _classifier_ should be instantiated by the `knn_predict` function that is implemented above. In general, the _classifier_ function should conform with a prescribed protocol of prototype, i.e., what the input and output arguments are.  For example its inputs are `(X_test, X_train, Y_train, k)` and its output is `Y_pred`.\n",
    "\n",
    "**Hint:** You may need to know how to use [*args](https://book.pythontips.com/en/latest/args_and_kwargs.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWHYG-ktFV0x"
   },
   "outputs": [],
   "source": [
    "def cross_validation(classifier, X, Y, n, *args):\n",
    "  \"\"\"\n",
    "  Perform cross validation for the given classifier, \n",
    "      and return the cross validation error rate.\n",
    "  Inputs:\n",
    "  - classifier: function of classification method\n",
    "  - X: A 2-D numpy array of shape (num_train, dim_feat), containing the whole dataset\n",
    "  - Y: A 1-D numpy array of length num_train, containing the ground-true labels\n",
    "  - n: number of folds\n",
    "  - *args: parameters needed by the classifier.\n",
    "        In this assignment, there is only one parameter (k) for the kNN clasifier.\n",
    "        For other classifiers, there may be multiple paramters. \n",
    "        To keep this function general, \n",
    "        let's use *args here for an unspecified number of paramters.\n",
    "  Output:\n",
    "  - error_rate: a floating-point number indicating the cross validation error rate\n",
    "  \"\"\"\n",
    "  # Here is the pseudo code:\n",
    "  # errors = []\n",
    "  # size = X.shape[0] # get the number of examples\n",
    "  # train_sets, val_sets = split_nfold(size, n)  # call the split_nfold function\n",
    "  #\n",
    "  # for i in range(n):\n",
    "  #   train_index = train_sets[i]\n",
    "  #   val_index = val_sets[i]\n",
    "  #   # get the training and validation sets of input features from X\n",
    "  # \tX_train, X_val = X[...], X[...] \n",
    "  #\n",
    "  #   # get the training and validation labels from Y\n",
    "  # \ty_train, y_val = Y[...], Y[...] \n",
    "  #\n",
    "  #   # call the classifier to get prediction results for the current validation set\n",
    "  # \typred = # call classifier with X_val, X_train, y_train, and *args\n",
    "  #                                   \n",
    "  # \terror = # call compute_error_rate to compute the error rate by comparing ypred against y_val\n",
    "  # \tappend error to the list `errors`\n",
    "  # error_rate = mean of errors\n",
    "  np.random.seed(1)\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  return error_rate \n",
    "\n",
    "# Unit test code here (you can uncomment the lines below to test)\n",
    "# X_dataset = np.array([[1, 2], [0, 3], [-1, 1], [-1, 0], [2, 1]])\n",
    "# Y_dataset = np.array([1, 1, 1, 0, 0])\n",
    "# n = 5\n",
    "# k = 3\n",
    "# cross_validation(knn_predict, X_dataset, Y_dataset, n, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZKSHAuf_R9U"
   },
   "source": [
    "Side note: instead of `for i in range(n):`, you can also use\n",
    "\n",
    "`for (train_index, val_index) in zip(train_sets, val_sets):`\n",
    "\n",
    "Try it if you like as it can be more generic. No need to submit anything for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNJ62NKozii3"
   },
   "source": [
    "## Problem 2: Optical character recognition (OCR) {-}\n",
    "\n",
    "We will now apply the above developed function to a real world problem of optical character recognition (OCR).\n",
    "\n",
    "**Load the MNIST dataset.** In the following code block, we have downloaded the MNIST dataset and split the data into trainning and test sets. This part has already been done, and you can directly run it with no need of modifying the code.  But do make sure that you understand the code as it will be useful for future labs.\n",
    "\n",
    "**Note:** after running the code, the training data (Xtrain, ytrain) has 10,000 examples, and the test data (Xtest, ytest) also has 10,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgWufXl41uJX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "DATA_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "\n",
    "# Download and import the MNIST dataset from Yann LeCun's website.\n",
    "# Each image is an array of 784 (28x28) float values  from 0 (white) to 1 (black).\n",
    "def load_data():\n",
    "    x_tr = load_images('train-images-idx3-ubyte.gz')\n",
    "    y_tr = load_labels('train-labels-idx1-ubyte.gz')\n",
    "    x_te = load_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_te = load_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    return x_tr, y_tr, x_te, y_te\n",
    "\n",
    "def load_images(filename):\n",
    "    maybe_download(filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    return data.reshape(-1, 28 * 28) / np.float32(256)\n",
    "\n",
    "def load_labels(filename):\n",
    "    maybe_download(filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    return data\n",
    "\n",
    "# Download the file, unless it's already here.\n",
    "def maybe_download(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(DATA_URL + filename, filename)\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = load_data()\n",
    "\n",
    "train_size = 10000\n",
    "test_size  = 10000\n",
    "\n",
    "Xtrain = Xtrain[0:train_size]\n",
    "ytrain = ytrain[0:train_size]\n",
    "\n",
    "Xtest = Xtest[0:test_size]\n",
    "ytest = ytest[0:test_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-KNutaT130w"
   },
   "source": [
    "##2.1 Effect of different numbers of training examples\n",
    "\n",
    "**(10 points)** In the following code block, we will compute the classification error of the 1-NN ($k=1$) for the MNIST dataset by calling the `knn_predict` function. We will study does the error change with different number of training examples.\n",
    "\n",
    "**Tasks**: train on the **first** $ntr$ number of training examples in (Xtrain, ytrain) that is produced by the above data-loading code, where $ntr$ is varied in $\\{100, 1000, 2500, 5000, 7500, 10000\\}$.\n",
    "1. Print the test error rate for each of these values of $ntr$.  Note that the above data-loading code produces 10,000 test examples stored in (Xtest, ytest). Just use all of them for testing, i.e., fixing the test set size to 10000.\n",
    "2. Plot a figure where the $x$-axis is the above values of $ntr$, and the $y$-axis is the test error rate.\n",
    "\n",
    "Directly calling `knn_predict` with the training and test set may cost too much memory.  So we will classify the test examples in batches, i.e., divide the test set into `nbtaches` number of subsets/batches, and predict for the first batch, then second batch, etc. For example, with 30 test examples and 5 batches, we first use `knn_predict` to classify test examples 0...5, then 6...11, ..., and finally 26...29.\n",
    "\n",
    "**Hint:** you may refer [here](https://matplotlib.org/tutorials/introductory/pyplot.html) for how to plot in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbnD99tN3WIG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  nbatches must be an even divisor of test_size. Increase if you run out of memory \n",
    "if test_size > 1000:\n",
    "  nbatches = 50\n",
    "else:\n",
    "  nbatches = 5\n",
    "\n",
    "# Let us first set up the index of each batch. \n",
    "# After running the next line, 'batches' will be a 2D array sized nbatches-by-m,\n",
    "# where m = test_size / nbatches.\n",
    "# batches[i] stores the indices (out of 0...test_size-1) for the i-th batch\n",
    "# You can run 'print(batches[3])' etc to witness the value of 'batches'.\n",
    "batches = np.array_split(np.arange(test_size), nbatches)\n",
    "ypred = np.zeros_like(ytest)\n",
    "trial_sizes = [100, 1000, 2500, 5000, 7500, 10000]\n",
    "trials = len(trial_sizes)\n",
    "error_rates = [0]*trials\n",
    "k = 1\n",
    "\n",
    "# Here is the pseudo code:\n",
    "# \n",
    "# for t = 0 ... trials-1  # loop over different number of training examples\n",
    "# \ttrial_size = trial_sizes[t]\n",
    "# \ttrial_X = Xtrain[...] # extract trial_size number of training examples from the whole training set\n",
    "# \ttrial_Y = Ytrain[...] # extract the corresponding labels\n",
    "# \tfor i = 0…nbatches—1\n",
    "# \t\typred[...] = # call knn_predict to classify the i-th batch of test examples.\n",
    "#                  # You should use 'batches' to get the indices for batch i.\n",
    "#                  # Then store the predicted labels also in the corresponding\n",
    "#                  # elements of ypred, so that after the loop over i completes,\n",
    "#                  # ypred will hold exactly the predicted labels of all test examples.\n",
    "# \terror_rate[t] = # call compute_error_rate to compute the error rate by \n",
    "#                     comparing ypred against ytest\n",
    "#   print a line like '#tr = 100, error rate = 50.3%'\n",
    "# plot the figure:\n",
    "# f = plt.figure()\n",
    "# plt.plot(...)\n",
    "# plt.xlabel(...)\n",
    "# plt.ylabel(...)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XeimykK5pYe"
   },
   "source": [
    "##2.2 Effect of different number of cross validation folds\n",
    "\n",
    "**(10 points)** In the following code block, we will perform cross validation on 1-NN classification. Call the `knn_predict` and `cross_validation` functions you have implemented, and compute the cross validation error rate for the first **1000** training examples with different number of folds $n \\in \\{3, 10, 50, 100, 1000\\}$. Then **print** the error rate for each different $n$ and **plot** a figure where the $x$-axis is $n = \\{3, 10, 50, 100, 1000\\}$, and the $y$-axis is the $n$-fold cross validation error rate. \n",
    "\n",
    "**Note about terminology:** In Problem 1, we used the term _dataset_, and the $n$-fold partitioning was on the _dataset_.  Now in the current setting, these **1000** training examples correspond to the _dataset_. In other words, this Section 2.2 will **not** use the test examples loaded from Problem 2, nor the remaining $10000 - 1000 = 9000$ training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-BHBavY64nc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "size = 1000\n",
    "k = 1\n",
    "\n",
    "# Here is the pseudo code:\n",
    "#\n",
    "# get the feature/label of the first 'size' (i.e., 1000) number of training examples\n",
    "# cvXtrain = Xtrain[...]  \n",
    "# cvytrain = ytrain[...]  \n",
    "\n",
    "# trial_folds   = [3, 10, 50, 100, 1000]\n",
    "# trials = number of trials on #folds, i.e., get the length of trial_folds (=5)\n",
    "# cverror_rates = [0]*trials\n",
    "\n",
    "# for t = 0 ... trials-1\n",
    "# \terror_rate = # call the 'cross_validation' function to get the error rate \n",
    "#                #  for the current trial (of fold number)\n",
    "# \tcverror_rates[t] = error_rate\n",
    "#\n",
    "#   # print the error rate for the current trial.\n",
    "# \tprint('{:d}-folds error rate: {:.2f}%\\n'.format(trial_folds[t], error_rate)) \n",
    "#\n",
    "# plot the figure:\n",
    "# f = plt.figure()\n",
    "# plt.plot(...)\n",
    "# plt.xlabel(...)\n",
    "# plt.ylabel(...)\n",
    "# plt.show()\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqIJpMGSznO7"
   },
   "source": [
    "## Problem 3: Iris plant recognition {-}\n",
    "\n",
    "The iris dataset includes 3 iris species of 50 examples each, where each example recorded petal and sepal length. For convenience, we will use the built-in functions in `scikit-learn` library to load dataset and create data partitions. For this experiment, we will use $80\\%$ (120) examples for training and $20\\%$ (30) for testing. \n",
    "\n",
    "Actually, we have done this data preparation work for you. You can directly use the training set (*X_train*, *Y_train*) and test set (*X_test*, *Y_test*) for the experiments, where *X* is features and *Y* is labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTjfkSdYFQI2"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# loading iris dataset\n",
    "iris = load_iris()\n",
    "# split dataset into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBtnC-PuIeGz"
   },
   "source": [
    "### 3.1 Find the best $k$. {-}\n",
    "In Problem 2, we conducted the experiments by arbitrarily setting $k$ to 1.\n",
    "In fact, the value of $k$ has a considerable impact on the performance of kNN. We will now determine the best value of this hyperparameter with $10$-fold cross-validation.\n",
    "To specify, we will vary $k$ in the range (1, 100) in increments of 1.\n",
    "Then we will find the best $k$ in terms of the lowest validation error rate.\n",
    "For this question, you need to:\n",
    "* **(5 points)** Store the validation error for each $k$ in an array and report the value of the best $k$. \n",
    "* **(5 points)** Plot a curve that shows the validation error rates as $k$ increases. \n",
    "\n",
    "**Note about terminology:** In Problem 1, we used the term _dataset_, and the $n$-fold partitioning was on the _dataset_.  Now in the current setting, (X_strain, Y_train) loaded above correspond to the _dataset_. In other words, this Section 3.1 will **not** use the test examples loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPm48esA3xJG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is the pseudo code:\n",
    "# list_ks = 1,2,...,100\n",
    "# err_ks = 1D array of length 100\n",
    "# for k in list_ks:\n",
    "#   err_ks[k-1] = cross_validation under k \n",
    "# best_k = argmin(err_ks)+1\n",
    "# plot err_ks versus list_ks\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EysI5YBT3xJK"
   },
   "source": [
    "### 3.2 Evaluation on test set {-}\n",
    "Since we have found the best hyperparameters for KNN classifier, it's time to evaluate this method on test data.\n",
    "\n",
    "**Task (5 points):** Report the classification error of kNN on test data, where $k$ is the optimal one from Section 3.1 (break tie arbitrarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJVXqpO43xJL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is the pseudo code:\n",
    "# y_pred = knn_predict on X_test using X_train, Y_train, and best_k\n",
    "# use compute_error_rate to compute the error of y_pred compared with Y_test\n",
    "# Print the error rate with a line like 'The test error is x.y%'\n",
    "\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD58RUy73xJN"
   },
   "source": [
    "### 3.3 F-score measurement {-}\n",
    "So far we have mainly used classification accuracy to evaluate the performance of our model. As a performance measure, accuracy is inappropriate for imbalanced classification problems.\n",
    "An alternative is the F-score metrics.\n",
    "\n",
    "**Tasks**\n",
    "* **(5 points)** Implement the computation of the confusion matrix on test set using `y_test` and the prediction `y_pred` from Section 3.2.  You can compare your result with the one computed by [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to ensure your implementation is correct.\n",
    "* **(2 points)** Report the precision, recall, and F1-score for each class by using the built-in functions from [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html).\n",
    "* **(3 points)** Write your own code to compute the F1-score for the three classes, and make sure they match the f1-score column of the sklearn result.\n",
    "\n",
    "**Hint:**  All definitions of confusion matrix, precision, recall, and F1-score can be found in the slides for Chapter19: DESİGN AND ANALYSİS OF MACHİNE LEARNİNG EXPERİMENTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6myu3yF3xJN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nclass = len(np.unique(Y_test))  # should be 3. Just be more adaptive to data.\n",
    "cm = np.zeros((nclass, nclass), dtype=int)  # confusion matrix is integer valued\n",
    "\n",
    "# Here is the pseudo code for Task 1: \n",
    "# for t = 0...nte-1  # nte is the number of test examples\n",
    "#    cm[c1, c2] += 1  # c1 and c2 corresponds to the class of the t-th test example\n",
    "#                     # according to Y_test and y_pred, respectively\n",
    "#\n",
    "# Here is the pseudo code for Task 3:\n",
    "# Well, please consult the textbook, as I really hope you can do it yourself,\n",
    "# especially when the right answer is provided by sklearn for comparison\n",
    "\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdxWEwOp9X23"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w0TZVbzIQ3Z"
   },
   "source": [
    "# Submission Instruction {-}\n",
    "\n",
    "You're almost done! Take the following steps to finally submit your work.\n",
    "\n",
    "1. After executing all commands and completing this notebook, save your `Lab_1.ipynb` as a PDF file, named as `X_Y_UIN.pdf`, where `X` is your first name, `Y` is your last name, and `UIN` is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots. \n",
    "\n",
    "> * Print out all unit test case results before printing the notebook into a PDF.\n",
    "* If you use Colab, open this notebook in Chrome.  Then File -> Print -> set Destination to \"Save as PDF\".  If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn't work, try Firefox.\n",
    "* If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under *File->Print...->Open PDF in Preview*. When the PDF opens in Preview, you can use *Save...* to save it.\n",
    "* Sometimes, a figure that appears near the end of a page can get cut.  In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page.\n",
    "\n",
    "2. Upload `X_Y_UIN.pdf` to Gradescope under `Lab_1_Written`.\n",
    "\n",
    "3. A template of `Lab_1.py` has been provided.  For all functions in `Lab_1.py`, copy the corresponding code snippets you have written into it.  Do not copy any code of plotting figures. **Do NOT** change the function names.\n",
    "\n",
    "4. Zip `Lab_1.py` and `Lab_1.ipynb` (**2 files**) into a zip file named `X_Y_UIN.zip`. Suppose the two files are in the folder `Lab_1`.  Then zip up the two files inside the `Lab_1` folder.  Do NOT zip up the folder `Lab_1`. Submit this zip file to Gradescope under `Lab_1_Code`. \n",
    "\n",
    "5. The autograder on Gradscope will be open all the time. We designed some simple test cases to help you check wehther your functions are executable. You will see the results of running autograder once you submit your code. Please follow the error messages to debug. Since those simple test cases are designed for debugging, it does not guaranttee your solution will work well on the real dataset. It is your responsibility to make your code logically correct. Since all functions are tested in batch, the autograder might take a few minutes to run after submission.\n",
    "\n",
    "You can submit to Gradescope as often as you like. We will only consider your last submission before the deadline."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
